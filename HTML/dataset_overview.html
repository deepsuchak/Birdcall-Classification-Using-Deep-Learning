<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Overview</title>
    <link href='https://fonts.googleapis.com/css?family=GFS Didot' rel='stylesheet'>
    <link rel="stylesheet" href="../CSS/dataset_overview.css">
  </head>
  <body>
    <div id="mySidenav" class="sidenav">
    <a href="javascript:void(0)" class="closebtn" onclick="closeNav()">&times;</a>
    <a href="../index.html">Home</a>
    <label for="dataset-touch"><a>Dataset</a></label>
    <input type="checkbox" id="dataset-touch">
    <ul class="slide-down">
      <li><a href="dataset_overview.html">Overview</a></li>
      <li><a href="dataset_EDA.html">Exploratory Data Analysis (EDA)</a></li>
    </ul>
    <label for="model-touch"><a>Models</a></label>
    <input type="checkbox" id="model-touch">
    <ul class="slide-down">
      <li><a href="resnet_model.html">ResNet</a></li>
      <li><a href="efficientnet_model.html">EfficientNet</a></li>
      <li><a href="yamnet_model.html">YAMNet</a></li>
    </ul>
    <a href="results_and_analysis.html">Results and Analysis</a>
    <a href="team.html">About Us</a>
    </div>
  <span onclick="openNav()"><img src="../Images/menu.png" class="navbar-icon"></span>
  <div class="page-title"><span>Birdcall Classification<br>Using Deep Learning</span></div>
  <div class="main-content">
    <div class="content-title">Dataset Overview</div>
    <div class="intro-to-dataset">
      <div class="dataset-image"><img src="../Images/dataset_birds.jpg"></div>
      <p>We will utilize the <a href="https://www.kaggle.com/c/birdsong-recognition" target="_blank">Cornell Birdcall Dataset</a>, an extensive collection of audio recordings featuring a diverse array of bird species. This dataset encompasses recordings captured across various environments and conditions, ensuring a comprehensive representation of avian biodiversity. Each recording comes with metadata, including the species of bird vocalizing, facilitating the application of supervised learning techniques for birdcall recognition.</p>
      <h2>About the Dataset</h2>
      <p>The Cornell birdcall dataset is a comprehensive collection of audio recordings featuring a diverse array of bird species. It encompasses a total of 264 classes representing various bird species from around the world. These recordings are captured in diverse environments and conditions, ensuring a representative sample of avian biodiversity. Each recording in the dataset is meticulously annotated with metadata, providing valuable context for analysis and interpretation. The metadata includes information such as the species of bird vocalizing, the location where the recording was captured, the date of the recording, and the user who provided the recording. For the purposes of our machine learning models, we've randomly selected 40 classes from the dataset. This selection process ensures focus and efficiency in our training efforts while still maintaining a diverse representation of bird species. The Cornell birdcall dataset serves as a rich resource for training and evaluating machine learning models for bird species identification. By leveraging this dataset, researchers and conservationists can develop robust and accurate birdcall recognition tools that meet the demands of real-world conservation applications. Our goal is to harness the potential of this dataset to contribute to the advancement of bird conservation efforts worldwide. Through the development of innovative machine learning models, we aim to empower conservationists with the tools and insights needed to protect avian biodiversity for generations to come.</p>
      <h2>Metadata in train.csv</h2>
      <ul>
          <li><strong>ebird_code:</strong> A unique code assigned to each bird species. You can access detailed information about each species by appending its ebird_code to the URL <a href="https://ebird.org/species/">https://ebird.org/species/</a>, such as <a href="https://ebird.org/species/amecro">https://ebird.org/species/amecro</a> for the American Crow.</li>
          <li><strong>recodist:</strong> The user who contributed the recording.</li>
          <li><strong>location:</strong> The geographical location where the recording was captured. Given that some bird species may exhibit regional call 'dialects', geographic diversity in training data is crucial.</li>
          <li><strong>date:</strong> Recording dates offer temporal context, as some bird calls may vary seasonally. Seeking diversity across different time frames enriches the training dataset.</li>
          <li><strong>filename:</strong> The name of the associated audio file, facilitating easy retrieval and reference.</li>
      </ul>
      <h2>Audio Recordings in train_audio folder</h2>
      <p>The <code>train_audio</code> directory contains the actual audio recordings, generously shared by users of <a href="https://www.xeno-canto.org/">xenocanto.org</a>. These recordings offer a rich and varied source of bird vocalizations, enabling comprehensive training of machine learning models for bird species identification. By harnessing the Cornell birdcall dataset and its accompanying metadata, we aim to develop robust machine learning models capable of accurately recognizing bird species from their vocalizations. This endeavor contributes significantly to the advancement of bird conservation efforts worldwide.</p>
    </div>
  </div>
  <script src="../JS/index_script.js"></script>
</body>
</html>