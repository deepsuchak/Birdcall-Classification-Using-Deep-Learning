<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>YAMNet</title>
    <link rel="stylesheet" href="../CSS/yamnet_model.css">
    <link href='https://fonts.googleapis.com/css?family=GFS Didot' rel='stylesheet'>
  </head>
  <body>
    <div id="mySidenav" class="sidenav">
    <a href="javascript:void(0)" class="closebtn" onclick="closeNav()">&times;</a>
    <a href="../index.html">Home</a>
    <label for="dataset-touch"><a>Dataset</a></label>
    <input type="checkbox" id="dataset-touch">
    <ul class="slide-down">
      <li><a href="dataset_overview.html">Overview</a></li>
      <li><a href="dataset_EDA.html">Exploratory Data Analysis (EDA)</a></li>
    </ul>
    <label for="model-touch"><a>Models</a></label>
    <input type="checkbox" id="model-touch">
    <ul class="slide-down">
      <li><a href="resnet_model.html">ResNet</a></li>
      <li><a href="efficientnet_model.html">EfficientNet</a></li>
      <li><a href="yamnet_model.html">YAMNet</a></li>
    </ul>
    <a href="results_and_analysis.html">Results and Analysis</a>
    <a href="team.html">About Us</a>
  </div>
  <span onclick="openNav()"><img src="../Images/menu.png" class="navbar-icon"></span>
  <div class="page-title"><span>Birdcall Classification<br>Using Deep Learning</span></div>
  <div class="main-content">
  <div class="content-title">YAMNet</div>
  <img src="../Images/yamnet_architechture.png" alt="YAMNet Architecture" class="model-architecture-image">
  <h1>What is YAMNet?</h1>
  <p>YAMNet is a pretrained deep net that predicts 521 audio event classes based on the AudioSet-YouTube corpus, and employing the Mobilenet_v1 depthwise-separable convolution architecture.</p>
  <h2>Workflow/Pipeline Overview</h2>
    <p>The high-level solution for birdcall identification using EfficientNet involves a systematic workflow consisting of data preprocessing, model development, training, and evaluation phases.</p>

    <h3>1. Data Preprocessing</h3>
    <p>The process begins with audio data collection and preprocessing. Various audio
      recordings are gathered from the specified directory. These audio files are loaded and resampled to a target
      sampling rate of 44,100 Hz.
    </p>

    <h3>2. Feature Extraction with YAMNet</h3>
    <p>The core of the solution involves feature extraction using the YAMNet
      model to classify audio recordings into different classes. The feature extraction process consists of the
      following steps:</p>
    <ul>
        <li>Loading Audio: Audio files are loaded using soundfile library and resampled if necessary to the target sampling rate.</li>
        <li>Extracting Embeddings: Audio data is passed through the YAMNet model to extract embeddings.</li>
        <li>Padding or Truncating: The extracted embeddings are padded or truncated to a fixed length to ensure uniformity.</li>
    </ul>

    <h3>3. Training Process</h3>
    <p>After feature extraction, a Convolutional Neural Network (CNN) model is defined and trained using the extracted embeddings as input features.</p>
    <ul>
      <li>Defining the Model: A CNN model is defined with convolutional and pooling layers followed by global average pooling and dense layers.</li>
      <li>Compiling the Model: The model is compiled with appropriate optimizer, loss function, and evaluation metrics.</li>
      <li>Training the Model: The model is trained and validated using training and validation datasets respectively.</li>
    </ul>

    <h3>4. Model Evaluation</h3>
    <p>Once the model is trained, its performance is evaluated using the validation set. Evaluation
      metrics such as accuracy, precision, F1 score, and Matthews Correlation Coefficient (MCC) are computed to
      assess the model's effectiveness in identifying birdcalls.</p>

    <h2>Implementation Details</h2>
    <ul>
        <li>The solution is implemented in Python using TensorFlow, soundfile, and other libraries for audio processing and machine learning.</li>
        <li>YAMNet, a pre-trained deep learning model, is used for extracting embeddings from audio data.</li>
        <li>The CNN model is defined and trained using TensorFlow's Keras API, with the extracted embeddings as input features.</li>
        <li>Model evaluation involves comparing predicted labels with ground truth labels and computing various evaluation metrics to assess model performance.</li>
    </ul>  
  </div>
  <script src="../JS/index_script.js"></script>
</body>
</html>